---
layout: post
title: "CS231n Notes"
date: 2016-03-25 
tags:
---

[TOC]

### dropout 的概率参数

一个 dropout 层, 输入是一个 $(N, D)$ 的矩阵, 输出依然是一个 $(N, D)$ 的矩阵, 但矩阵中的每个元素有 $p$ 的概率被置为 0.

![Imgur](http://i.imgur.com/PZLLRFw.png?1)

简单代码如下:

```python
def dropout_train(x, p):
	mask = np.random.rand(*x.shape) < p
	out = x * mask
	return out
```

#### 为什么 dropout 有用 ?

- 每一次屏蔽掉的神经元 (mask) 是不同的, 一个 batch 得到一个 mask, 相当于一个新的 model.
- 如此训练多次, 相当于是多个训练模型的 ensemble. 

#### 关于期望的问题

- 假设一组神经元在测试时的输出为 $\mathrm{x}$, 那么在训练时, 经过一个 dropout 层, 输出的期望则变为了 $\mathrm{x} * p$.
- 我们需要得到的结果为:
  output at test time = expected output at training time 

因此可以有两种解决办法:
 
```python
def train(x, p):
	mask = np.random.rand(*x.shape) < p
	out = x * mask
	return out
def predit(x):
	return x * p
```

或者

```python
def train(x, p):
	mask = (np.random.rand(*x.shape) < p) / p
	out = x * mask
	return out
def predit(x):
	return x
```

### Batch Normalization

#### 算法
对于一个 $(N, d)$ 维的向量 (实际上是一个batch): $\mathrm{x} = (x^{(1)} \dots x^{(d)}) $, 我们将其正则化为 

![Imgur](http://i.imgur.com/jMTbqM2.png)

更新的算法:

![enter image description here](http://i.imgur.com/V1C8e9x.png)

反向传播的公式:

![enter image description here](http://i.imgur.com/549FZ22.png)

[Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift](http://arxiv.org/pdf/1502.03167v3.pdf)

#### Batch Normalization 的好处
- 改善了梯度在网络中的传播
- 允许更高的学习率
- 对初始化条件依赖更少
- 相当于一种 regularization, 可以减少对 dropout 的依赖 (一种可能)

### 参数更新

#### SGD

```python
x += learning_rate * dx
```

#### Momentum

```python
v = mu * v - learning_rate * dx # velocity
x += v
```

#### Nesterov Momentum update

<img src="http://i.imgur.com/ELQpjhF.png" width="300px">

```python
v_prev = v
v = mu * v - learning_rate * dx # velocity
x += -mu * v_prev + (1 + mu) * v
```

#### AdaGrad update

```python
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + 1e-7)
```
相当于对 dx 的每个元素 scale 了一次.

#### RMSProp update

```python
cache += decay_rate * cache + (1 - decay_rate) * (dx**2)
x += - learning_rate * dx / (np.sqrt(cache) + 1e-7)
```

#### Adam update [Kingma and Ba, 2014]

```python
m,v = #... initialize cache to zeros
for t in xrange(1, big_number):
	dx = # evaluated gradient
	m = beta1 * m + (1-beta1) * dx # update first moment
	v = beta2 * v + (1-beta2) * (dx**2) # update second moment
	mb = m/(1-beta1**t) # correct bias
	vb = v/(1-beta2**t) # correct bias
	x += - learning_rate * mb / (np.sqrt(vb) + 1e-7)

```

#### Learning rate must decay!

![enter image description here](http://i.imgur.com/O5LqWSJ.png)

可以有多种逐步减小学习率的方法:

- step decay
  $\alpha = k \alpha_0$
- exponential decay
  $\alpha = \alpha_0e^{-kt}$
- 1/t decay
  $\alpha = \alpha_0 / (1 + kt)$

#### 总结

- 通常情况下, 默认用 **Adam** 效果就很好
- 如果可以载入全部数据 (full batch updates), 可以使用 **L-BFGS**